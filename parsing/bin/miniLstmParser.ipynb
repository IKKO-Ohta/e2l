{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# small parser\n",
    "\n",
    "上位千個のデータセットでparserを訓練し、基本的なロジックが成立していることを示す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer import serializers\n",
    "from chainer import Variable\n",
    "from chainer import cuda\n",
    "import gensim\n",
    "import numpy as np\n",
    "from chainer import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Parser(chainer.Chain):\n",
    "    def __init__(self):\n",
    "        \n",
    "        def initConfig():\n",
    "            conf = {\"outputDim\":3,\"midOne\":100,\"midTwo\":50}\n",
    "            with open(\"../model/word2id.pkl\",\"rb\") as f:\n",
    "                word2id = pickle.load(f)\n",
    "                conf[\"rawInputDim\"] = len(word2id)\n",
    "            with open(\"../model/simple_act_map.pkl\",\"rb\") as f:\n",
    "                simple_act_map = pickle.load(f)\n",
    "                conf[\"simpleActionMap\"] = len(simple_act_map)\n",
    "            with open(\"../model/word2POS.pkl\",\"rb\") as f:\n",
    "                word2POS = pickle.load(f)\n",
    "                conf[\"LenPOS\"] = len(set(list(word2POS.values())))\n",
    "                conf[\"POSex\"] = conf[\"LenPOS\"] // 2\n",
    "            return conf\n",
    "    \n",
    "        confs = initConfig()\n",
    "        self.raw_input_dim = confs[\"rawInputDim\"]\n",
    "        self.output_dim = confs[\"outputDim\"]\n",
    "        self.action_len = confs[\"simpleActionMap\"]\n",
    "        #self.w2vdim = 300\n",
    "        self.POS_len, self.POS_ex = confs[\"LenPOS\"], confs[\"POSex\"]\n",
    "        self.midOne, self.midTwo = confs[\"midOne\"], confs[\"midTwo\"]\n",
    "        self.bufDim = self.midOne + self.POS_ex\n",
    "        self.stkDim = self.midOne * 2 + self.action_len\n",
    "        #self.embedWordPreFix = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "        #    '../model/GoogleNews-vectors-negative300.bin',binary=True)\n",
    "\n",
    "        super(Parser, self).__init__(\n",
    "            #embedWordOfStack = L.EmbedID(self.raw_input_dim, self.midOne),\n",
    "            embedWordId = L.EmbedID(self.raw_input_dim, self.midOne),\n",
    "            embedHistoryId = L.EmbedID(self.action_len, self.action_len),\n",
    "            embedActionId = L.EmbedID(self.action_len, self.action_len),\n",
    "            embedPOSId = L.EmbedID(self.POS_len, self.POS_ex),\n",
    "            U = L.Linear(self.stkDim, self.midOne),  # stkInput => lstm\n",
    "            V = L.Linear(self.bufDim, self.midOne),  # bufInput => lstm\n",
    "            LS = L.LSTM(self.midOne, self.midTwo),  # for the subtree\n",
    "            LA = L.LSTM(self.action_len, self.action_len),  # for the action history\n",
    "            LB = L.LSTM(self.midOne, self.midTwo),  # for the buffer\n",
    "            W = L.Linear(self.midTwo*2 + self.action_len, self.midTwo), # [St;At;Bt] => classifier\n",
    "            G = L.Linear(self.midTwo, self.output_dim)  # output\n",
    "    )\n",
    "\n",
    "\n",
    "    def minibatchTrains(self,trains):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        trains:{\n",
    "                x_i: {\n",
    "                    his: historyID INT,\n",
    "                    buf: {\n",
    "                        w, WordID INT\n",
    "                        wlm, pre-trained word2vec np.ndarray(dtype=np.float32)\n",
    "                        t, POS tag ID INT\n",
    "                        },\n",
    "                     stk:{\n",
    "                         h: HEAD pre-trained word2vec np.ndarray(dtype=np.float32)\n",
    "                         d: DEPENDENT pre-trained word2vec np.ndarray(dtype=np.float32)\n",
    "                         r: actionID tag INT\n",
    "                        }\n",
    "                }\n",
    "                x_i+1:{\n",
    "                    his: ...,\n",
    "                    ...,\n",
    "                }\n",
    "            }\n",
    "        return: minibatch his,buf,stk\n",
    "        \"\"\"\n",
    "        errorcnt = 0\n",
    "        hiss,bufs,stks = 0,0,0\n",
    "        for train in trains:\n",
    "            his, buf, stk = train[0], train[1], train[2]\n",
    "\n",
    "            # his\n",
    "            his = self.embedHistoryId(np.asarray([his],dtype=np.int32))\n",
    "            hiss = F.vstack([hiss,his]) if type(hiss) != int else his\n",
    "\n",
    "            # buf\n",
    "            if buf == [-1,-1,-1]:\n",
    "                buf = np.asarray([0 for i in range(self.bufDim)],dtype=np.float32)\n",
    "                buf = Variable(buf).reshape(1,self.bufDim)\n",
    "            else:\n",
    "                buf = F.concat(\n",
    "                        (self.embedWordId(np.asarray([buf[0]],dtype=np.int32)),\n",
    "                        self.embedPOSId(np.asarray([buf[2]],dtype=np.int32))))\n",
    "            bufs = F.vstack([bufs,buf]) if type(bufs) != int else buf\n",
    "\n",
    "            # stk\n",
    "            compose = 0\n",
    "            for elem in stk[::-1]:\n",
    "                \"\"\"\n",
    "                elem = -1\n",
    "                のスカラーで回ってくることがある：\n",
    "                ../auto/preprocessed/13/wsj_1353_21_0000000.pkl周辺で発生\n",
    "                \"\"\"\n",
    "                if type(compose) == int:\n",
    "                    try:\n",
    "                        edge = F.concat(\n",
    "                        (self.embedWordId(np.asarray([elem[0]],dtype=np.int32)),\n",
    "                        self.embedWordId(np.asarray([elem[1]],dtype=np.int32)),\n",
    "                        self.embedActionId(np.asarray([elem[2]],dtype=np.int32))))\n",
    "                    except:\n",
    "                        sys.stderr.write(\"---stk loading error---\")\n",
    "                        sys.stderr.write(\"--- stk := [[-1,-1,0]]\")\n",
    "                        errorcnt += 1\n",
    "                        edge = F.concat(\n",
    "                        (self.embedWordId(np.asarray([-1],dtype=np.int32)),\n",
    "                        self.embedWordId(np.asarray([-1],dtype=np.int32)),\n",
    "                        self.embedActionId(np.asarray([0],dtype=np.int32))))\n",
    "\n",
    "                    compose = self.U(edge)\n",
    "                    compose = F.relu(compose)\n",
    "                else:\n",
    "                    edge = F.concat((\n",
    "                        compose,\n",
    "                        self.embedWordId(np.asarray([elem[1]],dtype=np.int32)),\n",
    "                        self.embedActionId(np.asarray([elem[2]],dtype=np.int32))\n",
    "                    ))\n",
    "                    compose = self.U(edge)\n",
    "                    compose = F.relu(compose)\n",
    "\n",
    "            stks = F.vstack([stks, compose]) if type(stks) != int else compose\n",
    "\n",
    "        return hiss,bufs,stks\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.LS.reset_state()\n",
    "        self.LA.reset_state()\n",
    "        self.LB.reset_state()\n",
    "\n",
    "    def __call__(self, his, buf, stk, label):\n",
    "        \"\"\"\n",
    "        params:\n",
    "            his: {his}, {his}, {...}\n",
    "            buf: {w,wlm,t}, {...}, {...}\n",
    "            stk: {h,d,r}, {...}, {...}\n",
    "            label: y0,y1,y2 ...\n",
    "        \"\"\"\n",
    "        # apply V\n",
    "        buf = F.relu(self.V(buf))\n",
    "\n",
    "        # apply LSTMs\n",
    "        at = F.relu(self.LA(his))\n",
    "        st = F.relu(self.LS(stk))\n",
    "        bt = F.relu(self.LB(buf))\n",
    "\n",
    "        # final stage\n",
    "        h1 = F.concat((st, at, bt))\n",
    "        h2 = F.relu(self.W(h1))\n",
    "        h3 = F.relu(self.G(h2))\n",
    "        return F.softmax_cross_entropy(h3,label)\n",
    "\n",
    "\n",
    "    def pred(self, his, buf, stk):\n",
    "         # apply V\n",
    "        buf = F.relu(self.V(buf))\n",
    "\n",
    "        # apply LSTMs\n",
    "        at = F.relu(self.LA(his))\n",
    "        st = F.relu(self.LS(stk))\n",
    "        bt = F.relu(self.LB(buf))\n",
    "\n",
    "        # final stage\n",
    "        h1 = F.concat((st, at, bt))\n",
    "        h2 = F.relu(self.W(h1))\n",
    "        h3 = F.relu(self.G(h2))\n",
    "        return F.argmax(h3, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ヘルパー関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def composeMatrix(loader,model,test=False):\n",
    "    \"\"\"\n",
    "    loaderナシにする！！\n",
    "    全てをバッチ/ミニバッチ学習で行う。\n",
    "    データは全てpandasに抱え込む。抱え込んだ後、良い感じにtrain/testスプリットする。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if test == False:\n",
    "            sentence = loader.gen()\n",
    "        else:\n",
    "            sentence = loader.genTestSentence()\n",
    "    except IndexError:\n",
    "        print(\"---loader finished---\")\n",
    "        return 0\n",
    "\n",
    "    trains = [sentence[i][0] for i in range(len(sentence))]\n",
    "    labelVec = [sentence[i][1] for i in range(len(sentence))]\n",
    "    hisMat, bufMat, stkMat = model.minibatchTrains(trains)\n",
    "    labelVec = Variable(np.asarray(labelVec,dtype=np.int32))\n",
    "\n",
    "    return [hisMat,bufMat,stkMat,labelVec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../auto/preprocessed/train/ud-train-9_0000035.pkl\"\n",
    "with open(path,\"br\") as f:\n",
    "    test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1,\n",
       "  [-1, -1, -1],\n",
       "  [[139, 138, 0], [139, 137, 0], [139, 141, 1], [141, 140, 0], [141, 35, 0]]],\n",
       " 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backupModel(model,epoch,dirpath=\"../model/\"):\n",
    "    now = datetime.datetime.now()\n",
    "    modelName = \"../model/parserModel\" +\"_\"+ \"ep\" + str(epoch) +\"_\"+ now.strftime('%s') + \".mod\"\n",
    "    serializers.save_hdf5(modelName, model)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    correct, cnt = 0, 0\n",
    "    predList,goldList = [],[]\n",
    "    while(1):\n",
    "        d = composeMatrix(loader,model,test=True)\n",
    "        if d:\n",
    "            hisMat, bufMat, stkMat, testVec = d[0],d[1],d[2],d[3]\n",
    "            predcls = model.pred(hisMat,bufMat,stkMat)\n",
    "            for pred, test in zip(predcls, testVec):\n",
    "                predList.append(pred)\n",
    "                goldList.append(test)\n",
    "                if pred.data == test.data:\n",
    "                     correct += 1\n",
    "                cnt += 1\n",
    "            model.reset_state()\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    print(\"correct / cnt:\", correct, \"/\", cnt)\n",
    "    return np.asarray(predList,dtype=np.int32),np.asarray(goldList,dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Parser()\n",
    "optimizer = optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "model.reset_state()\n",
    "model.cleargrads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
